---
layout: post
title: Approximate Message Passing (AMP) Algorithm
description: Approximate Message Passing (AMP) algorithm is to solve under-determined problem, that is, to recover sparse signals from few samples. I will take sparse channel estimation as an example to describe how to implement AMP algorithm and compare it with the well-known OMP (orthogonal matching pursuit) algorithm.
category: blog
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

Approximate Message Passing (AMP) algorithm is to solve under-determined problems, that is, to recover sparse signals from few samples. I will take sparse channel estimation as an example to describe how to implement AMP algorithm and compare it with the well-known OMP (orthogonal matching pursuit) algorithm.

## Under-determined Problem (Example: Sparse Channel Estimation)
Let us denote the channel information vector as \\(\boldsymbol{h}\\), and sparse channel means that most of elements in \\(\boldsymbol{h}\\) are zero. Consider transmitting pilot signals \\(\boldsymbol{x}\\) through a sparse channel, and the received samples \\(\boldsymbol{y}\\) vector is the convolution betwwen signals \\(\boldsymbol{x}\\) and the channel information vector \\(\boldsymbol{h}\\). The equation of \\(\boldsymbol{y}\\) is expressed as following:

$$
\begin{bmatrix}
y_{1}\\
y_{2}\\
y_{3}\\
\vdots\\
y_{n}
\end{bmatrix}=
\begin{bmatrix}
x_{N} & x_{N-1} & x_{N-2} & \ldots & x_1 \\
x_{1} & x_{N} & x_{N-1} & \ldots & x_2\\
x_{2} & x_{1} & x_{N} & \ldots & x_3 \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{N-1} & x_{n-2} & x_{n-3} & \ldots & x_{n}
\end{bmatrix}\cdot
\begin{bmatrix}
h_{1}\\
h_{2}\\
h_{3} \\
\vdots\\
h_{N}
\end{bmatrix}
+ \boldsymbol{w}
$$

that is,

$$\boldsymbol{y}=\mathbf{A}\cdot\boldsymbol{h} +\boldsymbol{w}$$

where \\(\boldsymbol{w}\\) is assumed to be Gaussian noise vector and \\(\mathbf{A} (n\times N)\\) is called sening matrix, composed of shift pilot sigals. Note that the size of \\(\boldsymbol{y}\\) is \\(n \times 1\\), and the size of \\(\boldsymbol{x}\\) and \\(\boldsymbol{h}\\) are \\(N \times 1\\).

Our goal is to find the accurate solution of channel information vector \\(h\\) with knowing \\( \boldsymbol{y} \\) and \\(\boldsymbol{A} \\). We may think that it is requirable that we have condition \\(n \geq N\\) based on our knowledge of linear algebra. However, in this scenario, our focus is to have good estimation of the channel while we can still have high data rate, which means that we would like to estimate the channel with received samples \\(\boldsymbol{y}\\) as fewer as possible. Therefore, we try to find a good estimation of vector \\(\boldsymbol{h}\\) with condition that \\( n < N\\), and this is called under-determined problem.

## AMP Algorithm
The sparse channel estimation example above has a important feature that vector \\( \boldsymbol{h}\\) is sparse. Here, AMP algorithm is to solve under-determined problems given that the vector we want to estimate is sparse. AMP is derived from the graphical model theory and message passing algorithm, see reference [1], and it simplifies the procedures of message passing that requires tracking of \\( 2nN\\) messages. It works iteratively with having vector \\( \boldsymbol{y}\\) and matrix \\( \mathbf{A}\\) as inputs. Estimation result denoted by \\( \hat{\boldsymbol{h}}\\) will converge after several iterations with mean square error (MSE) smaller than \\( 10^{-4}\\). In detail, AMP has following steps:

1. Intialization: \\(\boldsymbol{z}^0 = \boldsymbol{y}, \hat{\boldsymbol{h}}^0 = \boldsymbol{0}, t=0\\)
2. _while_ \\( t<I\\) _do_
3. &nbsp; &nbsp; \\(\hat{\boldsymbol{h}}^{t+1} = \eta(\boldsymbol{A}^{T}\boldsymbol{z}^t+\hat{\boldsymbol{h}}^t;\hat{\tau}^t)\\)
4. &nbsp; &nbsp; \\(\boldsymbol{z}^t = \boldsymbol{y}-\boldsymbol{A}\hat{\boldsymbol{h}}^t+\frac{1}{\delta}\boldsymbol{z}^{t-1}<\eta'(\boldsymbol{A}^{T}\boldsymbol{z}^{t-1}+\hat{\boldsymbol{h}}^{t-1};\hat{\tau}^{t-1})>\\)
5. &nbsp; &nbsp; \\( t = t+1\\)
6. &nbsp; &nbsp; \\(\hat{\tau}^{t+1} = \sqrt{N_0+\left(\frac{\hat{\tau}^{t-1}}{\delta}<\eta'(\boldsymbol{A}^{T}\boldsymbol{z}^{t-1}+\hat{\boldsymbol{h}}^{t-1};\hat{\tau}^{t-1})>\right)^2}\\)
7. end while

where,
 * \\(I, t\\): number of iterations and current iteration number
 * \\(\boldsymbol{z}^t\\): current residual vector
 * \\(\hat{\boldsymbol{h}}^t \\): current estimate vector
 * \\(\eta(\cdot)\\): thresholding function (component-wise)
 * \\(\hat{\tau}^{t+1}\\): next thresholding level (scaler)
 * \\(\delta\\): undersampling parameter \\( \delta = \frac{n}{N} \\)
 * \\(N_0\\): noise variance
 
AMP starts with all-zero estimate vector. At each iteration step, the estimate result \\(\hat{\boldsymbol{h}}^t\\) is updated with applying a threshoulding function \\(\eta(\cdot)\\) whose purpose is to set small elements (which is smaller than current thresholding level \\(\hat{\tau}^{t}\\)) in \\(\hat{\boldsymbol{h}}^t\\) into zero to force sparsity. Then the thresholding level \\(\hat{\tau}^{t}\\) is updated and it will be smaller than that in the previous iteration. The following figures show how AMP converges through several iterations.

![Git Bash](/images/githubpages/AMP/AMP_iter.png)

The blue curve is what we want to estimate, and it is a vector of size \\(1\times 100\\) in which 10 of them are non-zero. The red curve is the estimate results. We can see that the estimate starts with all-zero state, and in each iteration some elements in the estimate are set to zero. The last figure show the estimate reachs the true value. In this example, \\(n = 10, N=100\\), no more than 100 iterations (around one second) are needed to make AMP converge.

## Comparing with OMP Algorithm
Orthogonal matching pursuit (OMP) is another algorithm which can solve under-determined problems, and it is widely used in signal/image processing, shape representation, recognition, etc. The details of this algorithm is in reference [2]. Here we compare AMP with OMP over phase transition performances for real vectors. In experiments,



## Reference
1. David L Donoho, Arian Maleki, and Andrea Montanari. How to design message passing algorithms for compressed sensing. preprint, 2011.
2. Joel A Tropp and Anna C Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Transactions on information theory, 53(12):4655â€“4666, 2007.

