---
layout: post
title: Algorithm：Approximate Message Passing 
description: Approximate Message Passing (AMP) algorithm is to solve under-determined problems, that is, to recover sparse signals from few samples. I will take sparse channel estimation as an example to describe how to implement AMP algorithm and compare it with the well-known OMP (orthogonal matching pursuit) algorithm.
category: blog
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

Approximate Message Passing (AMP) algorithm is to solve under-determined problems, that is, to recover sparse signals from few samples. I will take sparse channel estimation as an example to describe how to implement AMP algorithm and compare it with the well-known OMP (orthogonal matching pursuit) algorithm.

The full code is <a href = "https://github.com/danqing117/AMP">here</a>.

## Under-determined Problem (Example: Sparse Channel Estimation)
Let us denote the channel information vector as \\(\boldsymbol{h}\\). "Sparse channel" means that most of elements in \\(\boldsymbol{h}\\) are zero. Consider transmitting pilot signals \\(\boldsymbol{x}\\) through a sparse channel, and the received samples \\(\boldsymbol{y}\\) vector is the convolution between signals \\(\boldsymbol{x}\\) and the channel information vector \\(\boldsymbol{h}\\). The equation of \\(\boldsymbol{y}\\) is expressed as following:

$$
\begin{bmatrix}
y_{1}\\
y_{2}\\
y_{3}\\
\vdots\\
y_{n}
\end{bmatrix}=
\begin{bmatrix}
x_{N} & x_{N-1} & x_{N-2} & \ldots & x_1 \\
x_{1} & x_{N} & x_{N-1} & \ldots & x_2\\
x_{2} & x_{1} & x_{N} & \ldots & x_3 \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{N-1} & x_{n-2} & x_{n-3} & \ldots & x_{n}
\end{bmatrix}\cdot
\begin{bmatrix}
h_{1}\\
h_{2}\\
h_{3} \\
\vdots\\
h_{N}
\end{bmatrix}
+ \boldsymbol{w}
$$

that is,

$$\boldsymbol{y}=\mathbf{A}\cdot\boldsymbol{h} +\boldsymbol{w}$$

where \\(\boldsymbol{w}\\) is assumed to be Gaussian noise vector and \\(\mathbf{A} (n\times N)\\) is called sensing matrix, composed of shift pilot singals. Note that the size of \\(\boldsymbol{y}\\) is \\(n \times 1\\), and the size of \\(\boldsymbol{x}\\) and \\(\boldsymbol{h}\\) are \\(N \times 1\\).

Our goal is to find the accurate solution of channel information vector \\(\boldsymbol{h}\\) with knowing \\( \boldsymbol{y} \\) and \\(\boldsymbol{A} \\). We may think that it is requirable that \\(n \geq N\\) based on our knowledge of basic linear algebra. However, in this scenario, our focus is to have good estimation of the channel while we can still have high data rate, which means that we would like to estimate the channel with received samples \\(\boldsymbol{y}\\) as fewer as possible. Therefore, we try to find a good estimation of vector \\(\boldsymbol{h}\\) with condition that \\( n < N\\), and this is an under-determined problem.

## AMP Algorithm
The sparse channel estimation example above has an important feature that vector \\( \boldsymbol{h}\\) is sparse. Here, AMP algorithm is to solve under-determined problems given that the vector we want to estimate is sparse. AMP is derived from the graphical model theory and message passing algorithm, see reference [1], and it simplifies the procedures of message passing that requires tracking of \\( 2nN\\) messages. It works iteratively with having vector \\( \boldsymbol{y}\\) and matrix \\( \mathbf{A}\\) as inputs. Estimation result denoted by \\( \hat{\boldsymbol{h}}\\) will converge after several iterations with mean square error (MSE) smaller than \\( 10^{-4}\\). In detail, AMP has following steps:

1. Intialization: \\(\boldsymbol{z}^0 = \boldsymbol{y}, \hat{\boldsymbol{h}}^0 = \boldsymbol{0}, t=0\\)
2. _while_ \\( t<I\\) _do_
3. &nbsp; &nbsp; \\(\hat{\boldsymbol{h}}^{t+1} = \eta(\boldsymbol{A}^{T}\boldsymbol{z}^t+\hat{\boldsymbol{h}}^t;\hat{\tau}^t)\\)
4. &nbsp; &nbsp; \\(\boldsymbol{z}^t = \boldsymbol{y}-\boldsymbol{A}\hat{\boldsymbol{h}}^t+\frac{1}{\delta}\boldsymbol{z}^{t-1}<\eta'(\boldsymbol{A}^{T}\boldsymbol{z}^{t-1}+\hat{\boldsymbol{h}}^{t-1};\hat{\tau}^{t-1})>\\)
5. &nbsp; &nbsp; \\( t = t+1\\)
6. &nbsp; &nbsp; \\(\hat{\tau}^{t+1} = \sqrt{N_0+\left(\frac{\hat{\tau}^{t-1}}{\delta}<\eta'(\boldsymbol{A}^{T}\boldsymbol{z}^{t-1}+\hat{\boldsymbol{h}}^{t-1};\hat{\tau}^{t-1})>\right)^2}\\)
7. end while

where,
 * \\(I, t\\): number of iterations and current iteration number
 * \\(\boldsymbol{z}^t\\): current residual vector
 * \\(\hat{\boldsymbol{h}}^t \\): current estimate vector
 * \\(\eta(\cdot)\\): thresholding function (component-wise)
 * \\(\hat{\tau}^{t+1}\\): next thresholding level (scaler)
 * \\(\delta\\): under-sampling parameter \\( \delta = \frac{n}{N} \\)
 * \\(N_0\\): noise variance
 
AMP starts with all-zero estimate vector. At each iteration step, the estimate result \\(\hat{\boldsymbol{h}}^t\\) is updated with applying a thresholding function \\(\eta(\cdot)\\) whose purpose is to set small elements (which is smaller than current thresholding level \\(\hat{\tau}^{t}\\)) in \\(\hat{\boldsymbol{h}}^t\\) into zero to force sparsity. Then the thresholding level \\(\hat{\tau}^{t}\\) is updated and it will be smaller than that in the previous iteration. The following figures show how AMP converges through several iterations.

![Git Bash](/images/githubpages/AMP/AMP_iter.png)

The blue curve is what we want to estimate, and it is a vector of size \\(1\times 100\\) in which 10 of them are non-zero. The red curve is the estimate result. We can see that the estimate starts with all-zero state, and in each iteration some elements in the estimate are set to zero. The last figure show the estimate reaches the true value. In this example, \\(n = 10, N=100\\), no more than 100 iterations (around one second) are needed to make AMP converge.

## Comparing with OMP Algorithm
Orthogonal matching pursuit (OMP) is another algorithm which can solve under-determined problems, and it is widely used in signal/image processing, shape representation, recognition, etc. The details of this algorithm are in reference [2]. Here we compare AMP with OMP over phase transition performances for real vectors. The idea behind the phase transition curve is to see how much non-zero elements AMP & OMP can estimate at most in a specific under-sampling level. Therefore, based on reference [1], we need to plot a \\(\delta-\rho\\) curve, where \\(\rho\\) is sparsity parameter defined as \\(\frac{\text{number of non-zero elements}}{n}\\). Generally, larger \\(\delta\\) we have, larger \\(\rho\\) we can deal with. In another word, if we have more received samples, then we can estimate vector with more non-zero elements. In this experiment, we set sensing matrix \\(\mathbf{A}\\) as Gaussian matrix and \\(N=1000\\). Results are shown in the following:

![Git Bash](/images/githubpages/AMP/PH.png)

In figures above, each curve splits \\(\delta-\rho\\) plane into two regions. The region under each curve are successful region where algorithms can work with all (\\(\delta, \rho\\)) pairs in this region. On the contrary, algorithms cannot work successfully in the region above each curve. In figure (a), AMP is better than OMP for non-zero elements in set {-1,1}, however, OMP is better in figure (b) where non-zero elements are Rayleigh distributed. This result shows that OMP can more easily fail when non-zero elements have equal-power while AMP is not influenced by the distribution of non-zero elements. The answer why OMP suffers from equal-power non-zero entries can be found in reference [2]. Take some time to think about it if you are interested:)


## Reference
1. David L Donoho, Arian Maleki, and Andrea Montanari. How to design message passing algorithms for compressed sensing. preprint, 2011.
2. Joel A Tropp and Anna C Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Transactions on information theory, 53(12):4655–4666, 2007.

